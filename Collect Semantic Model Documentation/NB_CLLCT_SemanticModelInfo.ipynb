{"cells":[{"cell_type":"markdown","source":["# Collect Semantic Model information\n","This notebook is intended to collect information from all semantic models that are located in the referenced workspace. \n","All information collected will be saved to the connected lakehouse (default lakehouse of this notebook) and stored for analysis and reporting. \n","\n","##### What's included\n","The notebook loops over all semantic models in the workspace and collects information about: \n","- Tables\n","- Columns\n","- Relationships between tables\n","- Measures\n","\n","##### Not included\n","Any other additional model objects or pieces of information (like partitions, storage modes, field parameters, refresh policies etc) are currently out of scope and can be added in the future. \n","\n","##### Missing functionality / room for improvement\n","The current version of the notebook does not allow setups like slowly changing dimensions. Currently, all tables are truncated before collecting information. Therefore, it is not (yet) possible to travel in time and see how a definition of a measure changed of time (for example). \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52540bd9-b377-4a01-8ee0-296441480d73"},{"cell_type":"markdown","source":["#### Define parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c10f315f-2a46-4cfe-8cad-6de9dc323e01"},{"cell_type":"code","source":["workspace_name = \"Semantic Link for Power BI Folks\" # not used currently, scoped to current workspace.\n","lakehouse_name = \"ModelDocumenter\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:14.1062711Z","session_start_time":"2025-07-22T10:02:14.1073085Z","execution_start_time":"2025-07-22T10:02:25.84856Z","execution_finish_time":"2025-07-22T10:02:26.2837194Z","parent_msg_id":"fc8e0679-d852-4042-90c0-8844fa95068e"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c17e985e-17c6-4d97-8e23-da36b9e74f85"},{"cell_type":"markdown","source":["#### Import libraries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da825998-0bf3-4205-b6c3-dc66c9f60a69"},{"cell_type":"code","source":["# Set the bases\n","from datetime import datetime, timezone\n","from pyspark.sql.functions import col, row_number\n","from pyspark.sql.window import Window\n","run_timestamp = datetime.now(tz=timezone.utc).isoformat()\n","import pyspark.sql.functions as F\n","import sempy.fabric as fabric\n","import time"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:14.2042332Z","session_start_time":null,"execution_start_time":"2025-07-22T10:02:26.2858232Z","execution_finish_time":"2025-07-22T10:02:31.3802805Z","parent_msg_id":"4c156465-8241-4049-a4aa-a5a015dd6494"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd1307dd-c4cb-48aa-87ba-9cb571b35d3b"},{"cell_type":"markdown","source":["#### Clean-up lakehouse\n","This section cleans-up the lakehouse before new information is collected. \n","\n","<mark>**Note:** in the current version, there is no collection of historical information in the lakehouse due to the trucate that happens before collecting new information.  </mark>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9e62c43c-b7a7-4f98-8781-c166b94f85a1"},{"cell_type":"code","source":["# To avoid duplications and polution in lakehouse, truncate all tables\n","\n","# List all tables in the default database\n","tables_df = spark.sql(\"SHOW TABLES\")\n","\n","# Collect the table names\n","tables = tables_df.collect()\n","\n","# Iterate through each table and execute TRUNCATE statements\n","for table in tables:\n","    table_name = table.tableName\n","    print(f\"Truncating table: {table_name}\")\n","    \n","    # Execute TRUNCATE command\n","    spark.sql(f\" DELETE FROM {table_name}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:14.4433742Z","session_start_time":null,"execution_start_time":"2025-07-22T10:02:31.3822009Z","execution_finish_time":"2025-07-22T10:03:06.3506608Z","parent_msg_id":"b779ad12-e157-4948-b3ab-3f91aa3ef15d"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Truncating table: md_datasets\nTruncating table: md_tables\nTruncating table: md_columns\nTruncating table: md_relationships\nTruncating table: md_measures\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"686e7c42-12ae-4dcb-bc51-2794798b1174"},{"cell_type":"markdown","source":["#### Functions\n","As the information retrieved from the semantic models can contain spaces in table/column/object names, which are not allowed in lakehouses, these functions help to remove special characters from object names. \n","\n","The second function is created to save the retrieved information to the lakehouse and adding additional columns (such as semantic model name and id) to support scenarios in which relationships between the tables are desired (in case multiple semantic models are in scope)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4feb0d3d-4c43-45c1-825d-8640f13981ed"},{"cell_type":"code","source":["# Function to get rid of special characters in retrieved information\n","def remove_special_chars_from_col_names(dataframe, tokens = r\" .,;{}()\\=\"):\n","    cols_without_bad_characters = [\n","        col.translate({ord(token): None for token in tokens})\n","        for col in dataframe.columns\n","    ]\n","    return dataframe.toDF(*cols_without_bad_characters)\n","\n","# Function to save the result set to the destination lakehouse, and adding additional information to the resultset such as DatasetName, DatasetId and a timestamp\n","def save_table(dataframe, dataset_name, entity):\n","    if dataframe.empty: \n","        print(f\"no results found for {entity}\")\n","        return \n","    df = spark.createDataFrame(dataframe)\n","    df = remove_special_chars_from_col_names(df)\n","    df = df.withColumn(\"DatasetName\", F.lit(dataset_name))\n","    df = df.withColumn(\"DatasetId\", F.lit(dataset_id))\n","    df = df.withColumn(\"_load_datetime\", F.lit(run_timestamp))\n","    print(f\"Writing entity '{entity}' to table {lakehouse_name}.MD_{entity}\")\n","    df.write.mode(\"append\").saveAsTable(f\"{lakehouse_name}.MD_{entity}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:14.7224239Z","session_start_time":null,"execution_start_time":"2025-07-22T10:03:06.35311Z","execution_finish_time":"2025-07-22T10:03:06.6889832Z","parent_msg_id":"00877d5a-0ca7-4d54-be5d-d55d20db9ec1"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"480393b1-586e-4a49-976c-08fdc65a0ea7"},{"cell_type":"markdown","source":["## Main code\n","This section references above defined parameters and functions and collects the actual information. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cbaed18e-5fab-44b6-8bd8-4bf73078160a"},{"cell_type":"markdown","source":["#### Collect semantic models in workspace"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3313dd2-10a2-4561-9f5e-315c35faf618"},{"cell_type":"code","source":["# List all semantic models in workspace\n","semanticmodels = fabric.list_datasets()\n","semanticmodels[['Dataset Name', 'Dataset ID']] # in case filter add: .str.endswith('_SM')]\n","\n","# Create datasets table\n","df_datasets = spark.createDataFrame(semanticmodels)\n","#df_datasets.write.format(\"delta\").mode(\"overwrite\").save(lakehouse_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:14.8663334Z","session_start_time":null,"execution_start_time":"2025-07-22T10:03:06.6911061Z","execution_finish_time":"2025-07-22T10:03:27.7748406Z","parent_msg_id":"f299a80b-1be4-48c6-8f16-43bda5cbf85a"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21b9e54e-234a-4ee4-a70c-d654648dbf97"},{"cell_type":"code","source":["# Save datasets table to lakehouse\n","df_datasets = remove_special_chars_from_col_names(df_datasets)\n","df_datasets.write.mode(\"append\").saveAsTable(\"MD_datasets\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:14.987752Z","session_start_time":null,"execution_start_time":"2025-07-22T10:03:27.7767078Z","execution_finish_time":"2025-07-22T10:03:30.2715274Z","parent_msg_id":"fb4f0e9c-129b-4fa3-be1e-da65bd3aea2d"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c53c95dd-d345-4e7f-be7a-ea628e543fc3"},{"cell_type":"markdown","source":["#### Avoid duplicate rows in lakehouse\n","Scans the lakehouse table and remove duplicates from the semantic models / datasets table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"193e268c-1001-45c8-a06f-d0e6f7c5ab9a"},{"cell_type":"code","source":["# Clean-up dataset list\n","\n","# Get current list of datasets\n","cleandatsets = spark.sql(f\"SELECT * FROM {lakehouse_name}.md_datasets\")\n","cleandatasetsrowcount = cleandatsets.count()\n","\n","#from pyspark.sql.functions import col, row_number\n","#from pyspark.sql.window import Window\n","\n","# Define the window specification to partition by Dataset ID and order by Last Update in descending order\n","window_spec = Window.partitionBy(\"DatasetID\").orderBy(col(\"LastUpdate\").desc())\n","\n","# Add a row number column to identify the most recent record\n","df_with_row_number = cleandatsets.withColumn(\"row_number\", row_number().over(window_spec))\n","\n","# Filter to keep only the most recent record for each Dataset ID\n","df_no_duplicates = df_with_row_number.filter(col(\"row_number\") == 1).drop(\"row_number\")\n","noduplicatesrowcount = df_no_duplicates.count()\n","\n","# Show rowcount diff\n","print(f\"Original row count: {cleandatasetsrowcount}, cleaned set: {noduplicatesrowcount}\")\n","\n","# save updated datasets list\n","df_no_duplicates.write.mode(\"overwrite\").saveAsTable(\"MD_datasets\")\n","updatedrowcount = spark.sql(f\"SELECT COUNT(*) AS VALIDATEDROWCOUNTAFTERUPDATE FROM {lakehouse_name}.md_datasets\")\n","print(\"Table cleaned successfully\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:15.1053041Z","session_start_time":null,"execution_start_time":"2025-07-22T10:03:30.2736582Z","execution_finish_time":"2025-07-22T10:03:38.3801732Z","parent_msg_id":"ad3b1484-756b-4550-940e-4ed8a3a6d97e"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Original row count: 8, cleaned set: 8\nTable cleaned successfully\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38e29905-93bb-497a-b179-abc3735a23bd"},{"cell_type":"markdown","source":["#### Collect additional information\n","This section loops over each semantic model in the workspace (as collected before) and collects additional information, such as: \n","- Table information\n","- Column information\n","- Relationship information\n","- Measure information\n","\n","Each of these resultsets is saved in a separate table in the lakehouse (using the earlier defined functions). \n","\n","##### Missing information\n","Some semantic models (such as default semantic models) may not contain all pieces of information or a semantic model might not have measures for example. The logging of this step will always return what it found in the semantic model and will inform in case there are no results. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a029d42c-9d19-4f39-bbfd-dcc69cd4a2b6"},{"cell_type":"code","source":["# For each semantic model\n","for index, row in semanticmodels.iterrows():\n","\n","    # Extract dataset_name and dataset_id from the current row\n","    dataset_name = row['Dataset Name']\n","    dataset_id = row['Dataset ID']\n","\n","    # Print name and datset id to screen at start\n","    print(f\"Starting Dataset Name: {dataset_name}, Dataset ID: {row['Dataset ID']}...\")\n","\n","    # Get semantic model meta data and save to lakehouse\n","    save_table(fabric.list_tables(dataset_name), dataset_name, \"tables\")\n","    save_table(fabric.list_tables(dataset_name, include_columns=True), dataset_name, \"columns\")\n","    save_table(fabric.list_relationships(dataset_name), dataset_name, \"relationships\")\n","    save_table(fabric.list_measures(dataset_name), dataset_name, \"measures\")\n","\n","    # Print name and dataset id to screen after completion\n","    print(f\"...Finished Dataset Name: {dataset_name}, Dataset ID: {dataset_id}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"9aad1dcf-1206-499e-b3fa-8ffaa9520779","normalized_state":"finished","queued_time":"2025-07-22T10:02:15.2195249Z","session_start_time":null,"execution_start_time":"2025-07-22T10:03:38.3825748Z","execution_finish_time":"2025-07-22T10:04:15.5013172Z","parent_msg_id":"d5400c6f-8cd9-4c78-806f-64db0cf94a2f"},"text/plain":"StatementMeta(, 9aad1dcf-1206-499e-b3fa-8ffaa9520779, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting Dataset Name: SempyDemo, Dataset ID: 20512a78-371d-4a9f-a6cc-c806ee080040...\nno results found for tables\nno results found for columns\nno results found for relationships\nno results found for measures\n...Finished Dataset Name: SempyDemo, Dataset ID: 20512a78-371d-4a9f-a6cc-c806ee080040\nStarting Dataset Name: Example Semantic Model, Dataset ID: 8ef42108-5df7-4222-85ee-76d8b8c40bc5...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nWriting entity 'relationships' to table ModelDocumenter.MD_relationships\nWriting entity 'measures' to table ModelDocumenter.MD_measures\n...Finished Dataset Name: Example Semantic Model, Dataset ID: 8ef42108-5df7-4222-85ee-76d8b8c40bc5\nStarting Dataset Name: ModelDocumentation, Dataset ID: e26c306a-7c56-4ddc-a1e5-28ca7b2bfa47...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nWriting entity 'relationships' to table ModelDocumenter.MD_relationships\nno results found for measures\n...Finished Dataset Name: ModelDocumentation, Dataset ID: e26c306a-7c56-4ddc-a1e5-28ca7b2bfa47\nStarting Dataset Name: AW2020IncrementalCompleted, Dataset ID: 6ec39195-812a-4a65-af95-93b2fd56f846...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nWriting entity 'relationships' to table ModelDocumenter.MD_relationships\nWriting entity 'measures' to table ModelDocumenter.MD_measures\n...Finished Dataset Name: AW2020IncrementalCompleted, Dataset ID: 6ec39195-812a-4a65-af95-93b2fd56f846\nStarting Dataset Name: IncrementalRefreshPartitioning, Dataset ID: 03438f61-978f-400a-b266-41a1867585c1...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nno results found for relationships\nWriting entity 'measures' to table ModelDocumenter.MD_measures\n...Finished Dataset Name: IncrementalRefreshPartitioning, Dataset ID: 03438f61-978f-400a-b266-41a1867585c1\nStarting Dataset Name: ModelDocumenter, Dataset ID: 4c74a146-668b-419f-a1d3-caaacb774f5a...\nno results found for tables\nno results found for columns\nno results found for relationships\nno results found for measures\n...Finished Dataset Name: ModelDocumenter, Dataset ID: 4c74a146-668b-419f-a1d3-caaacb774f5a\nStarting Dataset Name: LH_STORE_BPA, Dataset ID: 38f5af6c-da64-4a8e-9bad-2f676824ba42...\nno results found for tables\nno results found for columns\nno results found for relationships\nno results found for measures\n...Finished Dataset Name: LH_STORE_BPA, Dataset ID: 38f5af6c-da64-4a8e-9bad-2f676824ba42\nStarting Dataset Name: ModelBPA, Dataset ID: b9bbccd6-e49c-4f3d-94d5-a6e31ca3e612...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nno results found for relationships\nWriting entity 'measures' to table ModelDocumenter.MD_measures\n...Finished Dataset Name: ModelBPA, Dataset ID: b9bbccd6-e49c-4f3d-94d5-a6e31ca3e612\n"]}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f44f33a1-c437-495c-ae9c-70cb7e679bcf"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"2c3ed71f-917b-46b1-9bd8-bed8384b7bb8","default_lakehouse_name":"ModelDocumenter","default_lakehouse_workspace_id":"a63d0986-6351-414f-8807-7b2a2d53a487"}}},"nbformat":4,"nbformat_minor":5}