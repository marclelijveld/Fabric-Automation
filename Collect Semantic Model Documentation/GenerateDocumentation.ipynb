{"cells":[{"cell_type":"code","source":["# Set the bases\n","from datetime import datetime, timezone\n","run_timestamp = datetime.now(tz=timezone.utc).isoformat()\n","\n","workspace_name = \"Semantic Link for Power BI Folks\" # not used currently, scoped to current workspace.\n","lakehouse_name = \"ModelDocumenter\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92","normalized_state":"finished","queued_time":"2024-09-03T14:00:33.0747849Z","session_start_time":"2024-09-03T14:00:33.3058002Z","execution_start_time":"2024-09-03T14:00:43.2015679Z","execution_finish_time":"2024-09-03T14:00:46.1845363Z","parent_msg_id":"cb3804d8-33ce-4d77-8fca-dabe74ae279e"},"text/plain":"StatementMeta(, 0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd1307dd-c4cb-48aa-87ba-9cb571b35d3b"},{"cell_type":"code","source":["# To avoid duplications and polution in lakehouse, truncate all tables\n","\n","# List all tables in the default database\n","tables_df = spark.sql(\"SHOW TABLES\")\n","\n","# Collect the table names\n","tables = tables_df.collect()\n","\n","# Iterate through each table and execute TRUNCATE statements\n","for table in tables:\n","    table_name = table.tableName\n","    print(f\"Truncating table: {table_name}\")\n","    \n","    # Execute TRUNCATE command\n","    spark.sql(f\" DELETE FROM {table_name}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92","normalized_state":"finished","queued_time":"2024-09-03T14:00:33.0762945Z","session_start_time":null,"execution_start_time":"2024-09-03T14:00:46.7308098Z","execution_finish_time":"2024-09-03T14:01:01.194156Z","parent_msg_id":"d0d62806-e9bd-4913-9dd1-3274402c7aa8"},"text/plain":"StatementMeta(, 0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"686e7c42-12ae-4dcb-bc51-2794798b1174"},{"cell_type":"code","source":["# Define functions\n","import pyspark.sql.functions as F\n","import sempy.fabric as fabric\n","import time\n","\n","def remove_special_chars_from_col_names(dataframe, tokens = r\" .,;{}()\\=\"):\n","    cols_without_bad_characters = [\n","        col.translate({ord(token): None for token in tokens})\n","        for col in dataframe.columns\n","    ]\n","    return dataframe.toDF(*cols_without_bad_characters)\n","\n","def save_table(dataframe, dataset_name, entity):\n","    if dataframe.empty: \n","        print(f\"no results found for {entity}\")\n","        return \n","    df = spark.createDataFrame(dataframe)\n","    df = remove_special_chars_from_col_names(df)\n","    df = df.withColumn(\"DatasetName\", F.lit(dataset_name))\n","    df = df.withColumn(\"DatasetId\", F.lit(dataset_id))\n","    df = df.withColumn(\"_load_datetime\", F.lit(run_timestamp))\n","    print(f\"Writing entity '{entity}' to table {lakehouse_name}.MD_{entity}\")\n","    df.write.mode(\"append\").saveAsTable(f\"{lakehouse_name}.MD_{entity}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92","normalized_state":"finished","queued_time":"2024-09-03T14:00:33.0770255Z","session_start_time":null,"execution_start_time":"2024-09-03T14:01:01.8101402Z","execution_finish_time":"2024-09-03T14:01:18.4892442Z","parent_msg_id":"a7ae91ac-3ddc-4d9b-bb8e-62d472b116d3"},"text/plain":"StatementMeta(, 0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"480393b1-586e-4a49-976c-08fdc65a0ea7"},{"cell_type":"code","source":["# List all semantic models in workspace\n","semanticmodels = fabric.list_datasets()\n","semanticmodels[['Dataset Name', 'Dataset ID']] # in case filter add: .str.endswith('_SM')]\n","\n","# Create datasets table\n","df_datasets = spark.createDataFrame(semanticmodels)\n","#df_datasets.write.format(\"delta\").mode(\"overwrite\").save(lakehouse_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92","normalized_state":"finished","queued_time":"2024-09-03T14:00:33.0784486Z","session_start_time":null,"execution_start_time":"2024-09-03T14:01:18.9994332Z","execution_finish_time":"2024-09-03T14:01:25.5270468Z","parent_msg_id":"2b8acc15-b449-4fc9-b0ae-348d736f39d2"},"text/plain":"StatementMeta(, 0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21b9e54e-234a-4ee4-a70c-d654648dbf97"},{"cell_type":"code","source":["# Save datasets table to lakehouse\n","df_datasets = remove_special_chars_from_col_names(df_datasets)\n","df_datasets.write.mode(\"append\").saveAsTable(\"MD_datasets\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92","normalized_state":"finished","queued_time":"2024-09-03T14:00:33.0792201Z","session_start_time":null,"execution_start_time":"2024-09-03T14:01:26.0378257Z","execution_finish_time":"2024-09-03T14:01:31.0063196Z","parent_msg_id":"d7969bde-ab2e-4a65-955e-78aaa1004213"},"text/plain":"StatementMeta(, 0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c53c95dd-d345-4e7f-be7a-ea628e543fc3"},{"cell_type":"code","source":["# Clean-up dataset list\n","\n","# Get current list of datasets\n","cleandatsets = spark.sql(f\"SELECT * FROM {lakehouse_name}.md_datasets\")\n","cleandatasetsrowcount = cleandatsets.count()\n","\n","from pyspark.sql.functions import col, row_number\n","from pyspark.sql.window import Window\n","\n","# Define the window specification to partition by Dataset ID and order by Last Update in descending order\n","window_spec = Window.partitionBy(\"DatasetID\").orderBy(col(\"LastUpdate\").desc())\n","\n","# Add a row number column to identify the most recent record\n","df_with_row_number = cleandatsets.withColumn(\"row_number\", row_number().over(window_spec))\n","\n","# Filter to keep only the most recent record for each Dataset ID\n","df_no_duplicates = df_with_row_number.filter(col(\"row_number\") == 1).drop(\"row_number\")\n","noduplicatesrowcount = df_no_duplicates.count()\n","\n","# Show rowcount diff\n","print(f\"Original row count: {cleandatasetsrowcount}, cleaned set: {noduplicatesrowcount}\")\n","\n","# save updated datasets list\n","df_no_duplicates.write.mode(\"overwrite\").saveAsTable(\"MD_datasets\")\n","updatedrowcount = spark.sql(f\"SELECT COUNT(*) AS VALIDATEDROWCOUNTAFTERUPDATE FROM {lakehouse_name}.md_datasets\")\n","print(\"Table cleaned successfully\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92","normalized_state":"finished","queued_time":"2024-09-03T14:00:33.0799221Z","session_start_time":null,"execution_start_time":"2024-09-03T14:01:31.5246392Z","execution_finish_time":"2024-09-03T14:01:41.8444234Z","parent_msg_id":"420b349f-8b29-4c81-b3ca-f2a47cdc1893"},"text/plain":"StatementMeta(, 0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Original row count: 6, cleaned set: 6\nTable cleaned successfully\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38e29905-93bb-497a-b179-abc3735a23bd"},{"cell_type":"code","source":["# For each semantic model\n","for index, row in semanticmodels.iterrows():\n","\n","    # Extract dataset_name and dataset_id from the current row\n","    dataset_name = row['Dataset Name']\n","    dataset_id = row['Dataset ID']\n","\n","    # Print name and datset id to screen at start\n","    print(f\"Starting Dataset Name: {dataset_name}, Dataset ID: {row['Dataset ID']}...\")\n","\n","    # Get semantic model meta data and save to lakehouse\n","    save_table(fabric.list_tables(dataset_name), dataset_name, \"tables\")\n","    save_table(fabric.list_tables(dataset_name, include_columns=True), dataset_name, \"columns\")\n","    save_table(fabric.list_relationships(dataset_name), dataset_name, \"relationships\")\n","    save_table(fabric.list_measures(dataset_name), dataset_name, \"measures\")\n","\n","    # Print name and dataset id to screen after completion\n","    print(f\"...Finished Dataset Name: {dataset_name}, Dataset ID: {dataset_id}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92","normalized_state":"finished","queued_time":"2024-09-03T14:00:33.0814537Z","session_start_time":null,"execution_start_time":"2024-09-03T14:01:42.3806736Z","execution_finish_time":"2024-09-03T14:02:13.4804455Z","parent_msg_id":"000862ce-126a-4ddb-a380-222c2f26ad70"},"text/plain":"StatementMeta(, 0d7c956f-4123-4ec4-8c36-0a3b6cfe9c92, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting Dataset Name: SempyDemo, Dataset ID: 20512a78-371d-4a9f-a6cc-c806ee080040...\nno results found for tables\nno results found for columns\nno results found for relationships\nno results found for measures\n...Finished Dataset Name: SempyDemo, Dataset ID: 20512a78-371d-4a9f-a6cc-c806ee080040\nStarting Dataset Name: Example Semantic Model, Dataset ID: 8ef42108-5df7-4222-85ee-76d8b8c40bc5...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nWriting entity 'relationships' to table ModelDocumenter.MD_relationships\nWriting entity 'measures' to table ModelDocumenter.MD_measures\n...Finished Dataset Name: Example Semantic Model, Dataset ID: 8ef42108-5df7-4222-85ee-76d8b8c40bc5\nStarting Dataset Name: ModelDocumentation, Dataset ID: e26c306a-7c56-4ddc-a1e5-28ca7b2bfa47...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nWriting entity 'relationships' to table ModelDocumenter.MD_relationships\nno results found for measures\n...Finished Dataset Name: ModelDocumentation, Dataset ID: e26c306a-7c56-4ddc-a1e5-28ca7b2bfa47\nStarting Dataset Name: AW2020IncrementalCompleted, Dataset ID: 6ec39195-812a-4a65-af95-93b2fd56f846...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nWriting entity 'relationships' to table ModelDocumenter.MD_relationships\nWriting entity 'measures' to table ModelDocumenter.MD_measures\n...Finished Dataset Name: AW2020IncrementalCompleted, Dataset ID: 6ec39195-812a-4a65-af95-93b2fd56f846\nStarting Dataset Name: IncrementalRefreshPartitioning, Dataset ID: 03438f61-978f-400a-b266-41a1867585c1...\nWriting entity 'tables' to table ModelDocumenter.MD_tables\nWriting entity 'columns' to table ModelDocumenter.MD_columns\nno results found for relationships\nWriting entity 'measures' to table ModelDocumenter.MD_measures\n...Finished Dataset Name: IncrementalRefreshPartitioning, Dataset ID: 03438f61-978f-400a-b266-41a1867585c1\nStarting Dataset Name: ModelDocumenter, Dataset ID: 4c74a146-668b-419f-a1d3-caaacb774f5a...\nno results found for tables\nno results found for columns\nno results found for relationships\nno results found for measures\n...Finished Dataset Name: ModelDocumenter, Dataset ID: 4c74a146-668b-419f-a1d3-caaacb774f5a\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f44f33a1-c437-495c-ae9c-70cb7e679bcf"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"2c3ed71f-917b-46b1-9bd8-bed8384b7bb8","default_lakehouse_name":"ModelDocumenter","default_lakehouse_workspace_id":"a63d0986-6351-414f-8807-7b2a2d53a487"}}},"nbformat":4,"nbformat_minor":5}